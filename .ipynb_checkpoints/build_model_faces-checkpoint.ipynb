{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #install dependencies \n",
    "# ! pip install --upgrade pip\n",
    "# !pip install numpy --upgrade\n",
    "# ! pip install pandas --upgrade\n",
    "# ! pip install boto3 --upgrade\n",
    "# ! pip install requests --upgrade\n",
    "# ! pip install scikit-learn --upgrade\n",
    "# ! pip install tensorflow --upgrade\n",
    "# ! pip install keras --upgrade\n",
    "# ! pip install scikit-video --upgrade\n",
    "# ! pip install scikit-image --upgrade\n",
    "# !pip install sagemaker --upgrade\n",
    "# ! pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import cv2 as cv\n",
    "import os\n",
    "# import time\n",
    "import random \n",
    "import json\n",
    "from joblib import dump, load\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, Activation\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "face_detector = MTCNN()\n",
    "#sensitive variables in config.py file that is on .gitignore\n",
    "from config import key_, secret_, s3_bucket, kaggle_cookie\n",
    "\n",
    "from functions_for_testing import get_video_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meta.json') as m:\n",
    "    meta = json.load(m)\n",
    "#get list of videos that exist in my bucket\n",
    "video_df = pd.read_csv('video_information.csv')\n",
    "video_list = video_df['video_names'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame(video_link, skipped_frames=5):\n",
    "    '''\n",
    "    function that takes a link to a video, and returns the frame after 'skipped_frames' input variable\n",
    "    temporary function to prevent large amount of bucket queries -- combine with resize and detect image function later\n",
    "    '''\n",
    "    video = cv.VideoCapture(video_link)\n",
    "    frame_count = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    for skipped_frame in np.arange(0, (skipped_frames + 1)):\n",
    "        _ = video.grab()\n",
    "    _, frame = video.retrieve()\n",
    "    video.release()\n",
    "    return frame\n",
    "# look into improving this - 701 ms when loading from bucket, 50 ms when loading from file, 5 skipped frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_detect_face(frame, new_max_size=750, padding=(.1, 0.05, 0.05)):\n",
    "    '''\n",
    "    temporary function -- combine with grab frame later\n",
    "    -- want to reduce number of bucket queries--\n",
    "    inputs:\n",
    "    frame: a single frame or an image\n",
    "    new_max_size: the maximum size of the longer of the width/height the frame will be resized to prior\n",
    "    to looking for faces\n",
    "    padding: tuple of percentages; will be added to the size of the face to ensure the entire face is captured\n",
    "    -- the tuple is (top, bottom, horizontal)\n",
    "    the top param will move the top of the face by this param times the size of the face towards the top of the y axis\n",
    "    the bottom param will move the bottom of the face by this praram times the size of the face towards the bottom\n",
    "    the horizontal param will move the left and right edges of the face by this param towards the left and\n",
    "    right edges of the plane respectively\n",
    "    returns:\n",
    "    a list of arrays\n",
    "    each array is a cropped face with dimensions of 146 by 225 pixels\n",
    "    '''\n",
    "    #convert the frame to color\n",
    "    #unsure if this step is necessary, however cvtColor takes very little time (~200 Âµs )\n",
    "    img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    original_height = frame.shape[0]\n",
    "    original_width = frame.shape[1]\n",
    "    #get original shape of frame\n",
    "    original_height, original_width = frame.shape[0], frame.shape[1]\n",
    "    #get aspect ratio -- want to maintain this\n",
    "    img_size_ratio = original_height / original_width\n",
    "    #if the height is greater than the width, make new height the new_max_size, and\n",
    "    #make new width the new height divided by the aspect ratio\n",
    "    if original_height > original_width:\n",
    "        new_height = new_max_size\n",
    "        new_width = new_height / img_size_ratio\n",
    "    #otherwise, make the new width equal to the new max size, and \n",
    "    #the new height the new width times the aspect ratio\n",
    "    else:\n",
    "        new_width = new_max_size\n",
    "        new_height = new_width * img_size_ratio\n",
    "    #new dimensions -- the aspect ratio will not match exactly due to rounding, but will be close\n",
    "    new_dim = (int(new_width), int(new_height))\n",
    "    #resize the image while maintaining the aspect ratio, and changing the maximum edge length to new_max_size\n",
    "    resized_image = cv.resize(img, new_dim, interpolation = cv.INTER_AREA)\n",
    "    face_dictionaries = face_detector.detect_faces(resized_image)\n",
    "    faces = []\n",
    "    for face in range(len(face_dictionaries)):\n",
    "        #only review faces that have more than a 90% confidence of being a face\n",
    "        if face_dictionaries[face]['confidence'] > 0.9:\n",
    "            #the 'box' of the face is a list of pixel values as: '[x, y, width, height]'\n",
    "            box = face_dictionaries[face]['box']\n",
    "            #this is the left side of the face. This will look at the x 'box' value, and will move left by the \n",
    "            #percentage of the horizontal padding param\n",
    "            start_x = box[0] - (padding[2] * box[2])\n",
    "            #right side of the face. Will add the horizontal padding param to the width and add the result to the \n",
    "            #original x starting value\n",
    "            end_x = box[0] + ((1 + padding[2]) * box[2])\n",
    "            #bottom of face\n",
    "            start_y = box[1] - (padding[1] * box[3])\n",
    "            #top of face\n",
    "            end_y = box[1] + ((1 + padding[0]) * box[3])\n",
    "            #if the adjusted x starting value is negative, change the starting x value to 0 (the 0 index of the frame array)\n",
    "            if start_x < 0:\n",
    "                start_x = 0\n",
    "            if start_y < 0:\n",
    "                start_y = 0\n",
    "            #keep consistant - do additional research on this\n",
    "            face_ratio = 1.54 # will keep horizontal size the same (can experiment with adjusting the horizontal axis later)\n",
    "            #calculate the number of pixels the face is on the horizontal axis\n",
    "            x_size = end_x - start_x\n",
    "            #calculate the number of pixels the face is on the vertical axis\n",
    "            y_size = end_y - start_y\n",
    "            #get what y_size needs to be\n",
    "            y_size_with_ratio = x_size * face_ratio\n",
    "            #how much the y_size needs to be adjusted\n",
    "            y_size_change = y_size_with_ratio - y_size\n",
    "            start_y_ = start_y - y_size_change\n",
    "            end_y_ = end_y + y_size_change\n",
    "            if start_y_ < 0:\n",
    "                y_adjust = 0 - start_y_\n",
    "                end_y_ = min((end_y_ + y_adjust), resized_image.shape[0])\n",
    "                start_y_ = 0\n",
    "            elif end_y_ > resized_image.shape[0]:\n",
    "                y_adjust = end_y_ - resized_image.shape[0]\n",
    "                start_y_ = max(0, (start_y_ - y_adjust))\n",
    "                end_y_ = resized_image.shape[0]\n",
    "            start_x, end_x, start_y_, end_y_ = int(start_x), int(end_x), int(start_y_), int(end_y_)\n",
    "            face_image = resized_image[start_y_:end_y_, start_x:end_x]\n",
    "            new_dim_ = (146, 225) #hard coded - -will want to change if I update the _face_ratio\n",
    "            new_face = cv.resize(face_image, new_dim_, interpolation = cv.INTER_AREA)\n",
    "            faces.append(new_face)\n",
    "    return faces #this will eventually need to become an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_values(video, x_length, video_dictionary=meta):\n",
    "    '''\n",
    "    inputs:\n",
    "    video: video name\n",
    "    x_length: length of x input (the y value will need to have a similar length)\n",
    "    video_dictionary: dictionary that can be looked up to check if a video is real or fake\n",
    "    returns:\n",
    "    an array of y values that can be passed into a neural network\n",
    "    '''\n",
    "    #check if the video is fake\n",
    "    if video_dictionary[video]['label'] == 'FAKE':\n",
    "        #if so, the y_value is 0, otherwise it is 1\n",
    "        y_value = 0\n",
    "    else:\n",
    "        y_value = 1\n",
    "    #create a list with a len of x_length with the above y_value\n",
    "    y_values = []\n",
    "    for x in np.arange(0, x_length):\n",
    "        y_values.append(y_value)\n",
    "    #pass the above list to the to_categorical and the result can be passed into my model\n",
    "    y_values_ = to_categorical(y_values, num_classes=2)\n",
    "    return y_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = 'xmkwsnuzyq.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo -- build pipeline to pass data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\", activation = 'elu', input_shape=(225, 146,3)))\n",
    "#add more layers\n",
    "#must flatten before the output layer\n",
    "model.add(Flatten())\n",
    "#output layer\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=, \n",
    "         #params\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
