{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #install dependencies \n",
    "# ! pip install --upgrade pip\n",
    "# !pip install numpy --upgrade\n",
    "# ! pip install pandas --upgrade\n",
    "# ! pip install boto3 --upgrade\n",
    "# ! pip install requests --upgrade\n",
    "# ! pip install scikit-learn --upgrade\n",
    "# ! pip install tensorflow --upgrade\n",
    "# ! pip install keras --upgrade\n",
    "# ! pip install scikit-video --upgrade\n",
    "# ! pip install scikit-image --upgrade\n",
    "# !pip install sagemaker --upgrade\n",
    "# ! pip install opencv-python --upgrade\n",
    "# ! pip install MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import cv2 as cv\n",
    "import os\n",
    "# import time\n",
    "import random \n",
    "import json\n",
    "from joblib import dump, load\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import *# Dense, Flatten, Conv2D, Dropout, Activation, BatchNormalization, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "face_detector = MTCNN()\n",
    "#sensitive variables in config.py file that is on .gitignore\n",
    "from config import key_, secret_, s3_bucket, kaggle_cookie\n",
    "\n",
    "from functions_for_testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meta.json') as m:\n",
    "    meta = json.load(m)\n",
    "#get list of videos that exist in my bucket\n",
    "video_df = pd.read_csv('video_information.csv')\n",
    "video_list = video_df['video_names'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces_from_video(video_link, \n",
    "                         skipped_frames=15, \n",
    "                         new_max_size=750, \n",
    "                         face_confidence = 0.9,\n",
    "                         padding=(.15, 0.15, 0.15), #(.1, 0.05, 0.05) \n",
    "                         face_dim = (146, 225)):\n",
    "    '''\n",
    "    takes a link to a video as input, and returns an array of faces found in a single frame of the video\n",
    "    inputs:\n",
    "    \n",
    "    video_link: link to video that contains a frame you want to look at\n",
    "    skipped_frames: number of frames to skip before looking for faces\n",
    "    \n",
    "    face_confidence: the confidence, as a percentage, that the model used in MTCNN function needs to be in order \n",
    "    to treat a detected potential face as a face\n",
    "    \n",
    "    new_max_size: for the shorter of the length or width, the max size you want the frame to be resized to before\n",
    "    looking for faces\n",
    "    \n",
    "    padding: tuple of percentages; will be added to the size of the face to ensure the entire face is captured\n",
    "    -- the tuple is (top, bottom, horizontal)\n",
    "    the top param will move the top of the face by this param times the size of the face towards the top of the y axis\n",
    "    the bottom param will move the bottom of the face by this praram times the size of the face towards the bottom\n",
    "    the horizontal param will move the left and right edges of the face by this param towards the left and\n",
    "    right edges of the plane respectively\n",
    "    \n",
    "    returns:\n",
    "    an array of images (stored as an array) of found images in the frame in question\n",
    "    '''\n",
    "    #load the video\n",
    "    video = cv.VideoCapture(video_link)\n",
    "#     frame_count = int(video.get(cv.CAP_PROP_FRAME_COUNT)) #not needed, but takes very little runtime\n",
    "    #skip appropiate number of frames based on skipped_frames input\n",
    "    for skipped_frame in np.arange(0, (skipped_frames)):\n",
    "        _ = video.grab()\n",
    "    found_faces = False\n",
    "    while found_faces == False:\n",
    "        _ = video.grab()\n",
    "        _, frame = video.retrieve()\n",
    "        #convert the frame to color\n",
    "        #unsure if this step is necessary, however cvtColor takes very little time (~200 Âµs )\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        original_height = frame.shape[0]\n",
    "        original_width = frame.shape[1]\n",
    "        #get original shape of frame\n",
    "        original_height, original_width = frame.shape[0], frame.shape[1]\n",
    "        #get aspect ratio -- want to maintain this\n",
    "        img_size_ratio = original_height / original_width\n",
    "        #if the height is greater than the width, make new height the new_max_size, and\n",
    "        #make new width the new height divided by the aspect ratio\n",
    "        if original_height > original_width:\n",
    "            new_height = new_max_size\n",
    "            new_width = new_height / img_size_ratio\n",
    "        #otherwise, make the new width equal to the new max size, and \n",
    "        #the new height the new width times the aspect ratio\n",
    "        else:\n",
    "            new_width = new_max_size\n",
    "            new_height = new_width * img_size_ratio\n",
    "        #new dimensions -- the aspect ratio will not match exactly due to rounding, but will be close\n",
    "        new_dim = (int(new_width), int(new_height))\n",
    "        #resize the image while maintaining the aspect ratio, and changing the maximum edge length to new_max_size\n",
    "        resized_image = cv.resize(img, new_dim, interpolation = cv.INTER_AREA)\n",
    "        face_dictionaries = face_detector.detect_faces(resized_image)\n",
    "        faces = []\n",
    "        for face in range(len(face_dictionaries)):\n",
    "            #only review faces that have more than a face_confidence% confidence of being a face\n",
    "            if face_dictionaries[face]['confidence'] > face_confidence:\n",
    "                #the 'box' of the face is a list of pixel values as: '[x, y, width, height]'\n",
    "                box = face_dictionaries[face]['box']\n",
    "                #this is the left side of the face. This will look at the x 'box' value, and will move left by the \n",
    "                #percentage of the horizontal padding param\n",
    "                start_x = box[0] - (padding[2] * box[2])\n",
    "                #right side of the face. Will add the horizontal padding param to the width and add the result to the \n",
    "                #original x starting value\n",
    "                end_x = box[0] + ((1 + padding[2]) * box[2])\n",
    "                #bottom of face\n",
    "                start_y = box[1] - (padding[1] * box[3])\n",
    "                #top of face\n",
    "                end_y = box[1] + ((1 + padding[0]) * box[3])\n",
    "                #if the adjusted x starting value is negative, change the starting x value to 0 (the 0 index of \n",
    "                #the frame array)\n",
    "                if start_x < 0:\n",
    "                    start_x = 0\n",
    "                if start_y < 0:\n",
    "                    start_y = 0\n",
    "                #keep consistant - do additional research on this\n",
    "                face_ratio = round(face_dim[1] / face_dim[0], 2) # will keep horizontal size the same \n",
    "                #(can experiment with adjusting the horizontal axis later)\n",
    "                #calculate the number of pixels the face is on the horizontal axis\n",
    "                x_size = end_x - start_x\n",
    "                #calculate the number of pixels the face is on the vertical axis\n",
    "                y_size = end_y - start_y\n",
    "                #get what y_size needs to be\n",
    "                y_size_with_ratio = x_size * face_ratio\n",
    "                #how much the y_size needs to be adjusted\n",
    "                y_size_change = y_size_with_ratio - y_size\n",
    "                start_y_ = start_y - y_size_change\n",
    "                end_y_ = end_y + y_size_change\n",
    "                if start_y_ < 0:\n",
    "                    y_adjust = 0 - start_y_\n",
    "                    end_y_ = min((end_y_ + y_adjust), resized_image.shape[0])\n",
    "                    start_y_ = 0\n",
    "                elif end_y_ > resized_image.shape[0]:\n",
    "                    y_adjust = end_y_ - resized_image.shape[0]\n",
    "                    start_y_ = max(0, (start_y_ - y_adjust))\n",
    "                    end_y_ = resized_image.shape[0]\n",
    "                start_x, end_x, start_y_, end_y_ = int(start_x), int(end_x), int(start_y_), int(end_y_)\n",
    "                face_image = resized_image[start_y_:end_y_, start_x:end_x]\n",
    "                new_face = cv.resize(face_image, face_dim, interpolation = cv.INTER_AREA)#change new_dim_ to face_dim\n",
    "                faces.append(new_face)\n",
    "            if len(faces) > 0:\n",
    "                found_faces = True\n",
    "                video.release()\n",
    "                #if no faces are found with a confidence above face_confidence after the skipped_frames'th frame,\n",
    "                #the function will loop forever. I think the risk of this happening is low for the data set I am \n",
    "                #using. Can consider using a counter based on the number of frames if this happens\n",
    "    #convert faces list to array\n",
    "    faces_ = np.array(faces)\n",
    "    return faces_\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_values(video, video_dictionary=meta):\n",
    "    '''\n",
    "    inputs:\n",
    "    video: video name\n",
    "    \n",
    "    video_dictionary: dictionary that can be looked up to check if a video is real or fake\n",
    "    \n",
    "    returns:\n",
    "    arrays of x values, and y values that can be passed into a neural network\n",
    "    '''\n",
    "    #get the video link\n",
    "    try:\n",
    "        video_ = video.decode(\"utf-8\") #needed due to weirdness with tf.Dataset.from_generator\n",
    "    except AttributeError:\n",
    "        video_ = video\n",
    "    video_link = get_video_link(video_)\n",
    "    x_values = get_faces_from_video(video_link)\n",
    "    #check if the video is fake\n",
    "    if video_dictionary[video_]['label'] == 'FAKE':\n",
    "        #if so, the y_value is 0, otherwise it is 1\n",
    "        y_value = 0\n",
    "    else:\n",
    "        y_value = 1\n",
    "    #create a list with a len matching the len of x_values with the above y_value\n",
    "    y_values = []\n",
    "    for x in np.arange(0, len(x_values)):\n",
    "        y_values.append(y_value)\n",
    "    #pass the above list to the to_categorical and the result can be passed into my model\n",
    "    y_values_ = to_categorical(y_values, num_classes=2)\n",
    "    return x_values, y_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Sequence): \n",
    "    # Class that will allow multiprocessing\n",
    "    def __init__(self, video_list, y_set=None, batch_size=1):\n",
    "        #convert the video_list to an array\n",
    "        self.x, self.y = np.array(video_list), y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.x.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(self.x.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        #currently only accepts a batch size of 1, update \"idx\" to \"inds\" once can accept larger batch size\n",
    "        batch_x, batch_y = get_xy_values(video_list[idx]) \n",
    "        #consider allowing get_faces_from_video to accept a list of video names, and loop through the function\n",
    "        #for each video name. If I do this, I would also need to upgrade the get xy values function \n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(video_list):\n",
    "    '''\n",
    "    a python generator that yields x and y values to be passed into a neural network\n",
    "    \n",
    "    Inputs:\n",
    "    video_list: a list of video names\n",
    "    \n",
    "    yields:\n",
    "    x and y values intended to be passed into a neural network\n",
    "    \n",
    "    todo -- inprove above description\n",
    "    '''\n",
    "    video_list_ = video_list#['xmkwsnuzyq.mp4']\n",
    "    list_len = len(video_list_)\n",
    "    random.shuffle(video_list_)\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count == list_len:\n",
    "            random.shuffle(video_list_)\n",
    "            count = 0\n",
    "        x, y = get_xy_values(video_list_[count])\n",
    "        count += 1\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_videos = []\n",
    "fake_videos = []\n",
    "for video in meta:\n",
    "    if meta[video]['label'] == 'REAL':\n",
    "        real_videos.append(video)\n",
    "    else:\n",
    "        fake_videos.append(video)\n",
    "video_set = []\n",
    "for video in range(len(real_videos)):\n",
    "    video_set.append(real_videos[video])\n",
    "    video_set.append(fake_videos[video])\n",
    "#split videos into test and training \n",
    "_, _, train_videos, test_videos = train_test_split(video_set, video_set, test_size=.003, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Generator(train_videos, train_videos)\n",
    "test_generator = Generator(test_videos, test_videos)\n",
    "training_len = len(train_videos)\n",
    "testing_len = len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_ = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    args=[train_videos], \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=([None, 225, 146, 3], [None, 2])\n",
    ")\n",
    "test_generator_ = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    args=[test_videos], \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=([None, 225, 146, 3], [None, 2])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (5, 5), padding=\"same\", activation = 'relu', input_shape=(225, 146,3)))\n",
    "#add more layers\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (6, 6), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Conv2D(64, (8, 8), activation = 'relu'))\n",
    "model.add(MaxPooling2D(( 2, 2)))\n",
    "model.add(Conv2D(16, (4, 4), activation = 'relu'))\n",
    "model.add(MaxPooling2D(( 2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv2D(32, (4, 4), activation = 'relu'))\n",
    "model.add(MaxPooling2D(( 2, 2)))\n",
    "\n",
    "\n",
    "#must flatten before the output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(32))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "model.fit(x=train_generator_, \n",
    "          validation_data=test_generator_, \n",
    "          steps_per_epoch=training_len//batch_size,\n",
    "          validation_steps=testing_len//batch_size,\n",
    "          workers=8, \n",
    "          use_multiprocessing=True, \n",
    "          epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "model.fit(x=train_generator_, \n",
    "          validation_data=test_generator_, \n",
    "          steps_per_epoch=training_len//batch_size,\n",
    "          validation_steps=testing_len//batch_size,\n",
    "          workers=8, \n",
    "          use_multiprocessing=False, \n",
    "          epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_xy_values('xpzfhhwkwb.mp4') # fake video\n",
    "res_fake = model.predict(x)\n",
    "res_fake[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_xy_values('xmkwsnuzyq.mp4') # real video\n",
    "res_real = model.predict(x)\n",
    "res_real[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
