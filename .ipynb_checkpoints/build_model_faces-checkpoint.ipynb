{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #install dependencies \n",
    "# ! pip install --upgrade pip\n",
    "# !pip install numpy --upgrade\n",
    "# ! pip install pandas --upgrade\n",
    "# ! pip install boto3 --upgrade\n",
    "# ! pip install requests --upgrade\n",
    "# ! pip install scikit-learn --upgrade\n",
    "# ! pip install tensorflow --upgrade\n",
    "# ! pip install keras --upgrade\n",
    "# ! pip install scikit-video --upgrade\n",
    "# ! pip install scikit-image --upgrade\n",
    "# !pip install sagemaker --upgrade\n",
    "# ! pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import cv2 as cv\n",
    "import os\n",
    "# import time\n",
    "import random \n",
    "import json\n",
    "from joblib import dump, load\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import *# Dense, Flatten, Conv2D, Dropout, Activation, BatchNormalization, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "face_detector = MTCNN()\n",
    "#sensitive variables in config.py file that is on .gitignore\n",
    "from config import key_, secret_, s3_bucket, kaggle_cookie\n",
    "\n",
    "from functions_for_testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meta.json') as m:\n",
    "    meta = json.load(m)\n",
    "#get list of videos that exist in my bucket\n",
    "video_df = pd.read_csv('video_information.csv')\n",
    "video_list = video_df['video_names'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces_from_video(video_link, \n",
    "                         skipped_frames=15, \n",
    "                         new_max_size=750, \n",
    "                         face_confidence = 0.9,\n",
    "                         padding=(.15, 0.15, 0.15), #(.1, 0.05, 0.05) \n",
    "                         face_dim = (146, 225)):\n",
    "    '''\n",
    "    takes a link to a video as input, and returns an array of faces found in a single frame of the video\n",
    "    inputs:\n",
    "    \n",
    "    video_link: link to video that contains a frame you want to look at\n",
    "    skipped_frames: number of frames to skip before looking for faces\n",
    "    \n",
    "    face_confidence: the confidence, as a percentage, that the model used in MTCNN function needs to be in order \n",
    "    to treat a detected potential face as a face\n",
    "    \n",
    "    new_max_size: for the shorter of the length or width, the max size you want the frame to be resized to before\n",
    "    looking for faces\n",
    "    \n",
    "    padding: tuple of percentages; will be added to the size of the face to ensure the entire face is captured\n",
    "    -- the tuple is (top, bottom, horizontal)\n",
    "    the top param will move the top of the face by this param times the size of the face towards the top of the y axis\n",
    "    the bottom param will move the bottom of the face by this praram times the size of the face towards the bottom\n",
    "    the horizontal param will move the left and right edges of the face by this param towards the left and\n",
    "    right edges of the plane respectively\n",
    "    \n",
    "    returns:\n",
    "    an array of images (stored as an array) of found images in the frame in question\n",
    "    '''\n",
    "    #load the video\n",
    "    video = cv.VideoCapture(video_link)\n",
    "#     frame_count = int(video.get(cv.CAP_PROP_FRAME_COUNT)) #not needed, but takes very little runtime\n",
    "    #skip appropiate number of frames based on skipped_frames input\n",
    "    for skipped_frame in np.arange(0, (skipped_frames)):\n",
    "        _ = video.grab()\n",
    "    found_faces = False\n",
    "    while found_faces == False:\n",
    "        _ = video.grab()\n",
    "        _, frame = video.retrieve()\n",
    "        #convert the frame to color\n",
    "        #unsure if this step is necessary, however cvtColor takes very little time (~200 Âµs )\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        original_height = frame.shape[0]\n",
    "        original_width = frame.shape[1]\n",
    "        #get original shape of frame\n",
    "        original_height, original_width = frame.shape[0], frame.shape[1]\n",
    "        #get aspect ratio -- want to maintain this\n",
    "        img_size_ratio = original_height / original_width\n",
    "        #if the height is greater than the width, make new height the new_max_size, and\n",
    "        #make new width the new height divided by the aspect ratio\n",
    "        if original_height > original_width:\n",
    "            new_height = new_max_size\n",
    "            new_width = new_height / img_size_ratio\n",
    "        #otherwise, make the new width equal to the new max size, and \n",
    "        #the new height the new width times the aspect ratio\n",
    "        else:\n",
    "            new_width = new_max_size\n",
    "            new_height = new_width * img_size_ratio\n",
    "        #new dimensions -- the aspect ratio will not match exactly due to rounding, but will be close\n",
    "        new_dim = (int(new_width), int(new_height))\n",
    "        #resize the image while maintaining the aspect ratio, and changing the maximum edge length to new_max_size\n",
    "        resized_image = cv.resize(img, new_dim, interpolation = cv.INTER_AREA)\n",
    "        face_dictionaries = face_detector.detect_faces(resized_image)\n",
    "        faces = []\n",
    "        for face in range(len(face_dictionaries)):\n",
    "            #only review faces that have more than a face_confidence% confidence of being a face\n",
    "            if face_dictionaries[face]['confidence'] > face_confidence:\n",
    "                #the 'box' of the face is a list of pixel values as: '[x, y, width, height]'\n",
    "                box = face_dictionaries[face]['box']\n",
    "                #this is the left side of the face. This will look at the x 'box' value, and will move left by the \n",
    "                #percentage of the horizontal padding param\n",
    "                start_x = box[0] - (padding[2] * box[2])\n",
    "                #right side of the face. Will add the horizontal padding param to the width and add the result to the \n",
    "                #original x starting value\n",
    "                end_x = box[0] + ((1 + padding[2]) * box[2])\n",
    "                #bottom of face\n",
    "                start_y = box[1] - (padding[1] * box[3])\n",
    "                #top of face\n",
    "                end_y = box[1] + ((1 + padding[0]) * box[3])\n",
    "                #if the adjusted x starting value is negative, change the starting x value to 0 (the 0 index of \n",
    "                #the frame array)\n",
    "                if start_x < 0:\n",
    "                    start_x = 0\n",
    "                if start_y < 0:\n",
    "                    start_y = 0\n",
    "                #keep consistant - do additional research on this\n",
    "                face_ratio = round(face_dim[1] / face_dim[0], 2) # will keep horizontal size the same \n",
    "                #(can experiment with adjusting the horizontal axis later)\n",
    "                #calculate the number of pixels the face is on the horizontal axis\n",
    "                x_size = end_x - start_x\n",
    "                #calculate the number of pixels the face is on the vertical axis\n",
    "                y_size = end_y - start_y\n",
    "                #get what y_size needs to be\n",
    "                y_size_with_ratio = x_size * face_ratio\n",
    "                #how much the y_size needs to be adjusted\n",
    "                y_size_change = y_size_with_ratio - y_size\n",
    "                start_y_ = start_y - y_size_change\n",
    "                end_y_ = end_y + y_size_change\n",
    "                if start_y_ < 0:\n",
    "                    y_adjust = 0 - start_y_\n",
    "                    end_y_ = min((end_y_ + y_adjust), resized_image.shape[0])\n",
    "                    start_y_ = 0\n",
    "                elif end_y_ > resized_image.shape[0]:\n",
    "                    y_adjust = end_y_ - resized_image.shape[0]\n",
    "                    start_y_ = max(0, (start_y_ - y_adjust))\n",
    "                    end_y_ = resized_image.shape[0]\n",
    "                start_x, end_x, start_y_, end_y_ = int(start_x), int(end_x), int(start_y_), int(end_y_)\n",
    "                face_image = resized_image[start_y_:end_y_, start_x:end_x]\n",
    "                new_face = cv.resize(face_image, face_dim, interpolation = cv.INTER_AREA)#change new_dim_ to face_dim\n",
    "                faces.append(new_face)\n",
    "            if len(faces) > 0:\n",
    "                found_faces = True\n",
    "                video.release()\n",
    "                #if no faces are found with a confidence above face_confidence after the skipped_frames'th frame,\n",
    "                #the function will loop forever. I think the risk of this happening is low for the data set I am \n",
    "                #using. Can consider using a counter based on the number of frames if this happens\n",
    "    #convert faces list to array\n",
    "    faces_ = np.array(faces)\n",
    "    return faces_\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_values(video, video_dictionary=meta):\n",
    "    '''\n",
    "    inputs:\n",
    "    video: video name\n",
    "    \n",
    "    video_dictionary: dictionary that can be looked up to check if a video is real or fake\n",
    "    \n",
    "    returns:\n",
    "    arrays of x values, and y values that can be passed into a neural network\n",
    "    '''\n",
    "    #get the video link\n",
    "    video_link = get_video_link(video)\n",
    "    x_values = get_faces_from_video(video_link)\n",
    "    #check if the video is fake\n",
    "    if video_dictionary[video]['label'] == 'FAKE':\n",
    "        #if so, the y_value is 0, otherwise it is 1\n",
    "        y_value = 1\n",
    "    else:\n",
    "        y_value = 0\n",
    "    #create a list with a len matching the len of x_values with the above y_value\n",
    "    y_values = []\n",
    "    for x in np.arange(0, len(x_values)):\n",
    "        y_values.append(y_value)\n",
    "    #pass the above list to the to_categorical and the result can be passed into my model\n",
    "    y_values_ = to_categorical(y_values, num_classes=2)\n",
    "    return x_values, y_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = 'xmkwsnuzyq.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo -- build pipeline to pass data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Sequence): \n",
    "    # Class that will allow multiprocessing\n",
    "    def __init__(self, video_list, y_set=None, batch_size=1):\n",
    "        #convert the video_list to an array\n",
    "        self.x, self.y = np.array(video_list), y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.x.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(self.x.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        #currently only accepts a batch size of 1, update \"idx\" to \"inds\" once can accept larger batch size\n",
    "        batch_x, batch_y = get_xy_values(video_list[idx]) \n",
    "        #consider allowing get_faces_from_video to accept a list of video names, and loop through the function\n",
    "        #for each video name. If I do this, I would also need to upgrade the get xy values function \n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_videos = []\n",
    "fake_videos = []\n",
    "for video in meta:\n",
    "    if meta[video]['label'] == 'REAL':\n",
    "        real_videos.append(video)\n",
    "    else:\n",
    "        fake_videos.append(video)\n",
    "video_set = []\n",
    "for video in range(100):\n",
    "    video_set.append(real_videos[video])\n",
    "    video_set.append(fake_videos[video])\n",
    "#split videos into test and training \n",
    "_, _, train_videos, test_videos = train_test_split(video_set, video_set, test_size=.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Generator(train_videos, train_videos)\n",
    "test_generator = Generator(test_videos, test_videos)\n",
    "training_len = len(train_videos)\n",
    "testing_len = len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_181 (Conv2D)          (None, 225, 146, 8)       224       \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 225, 146, 8)       32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 112, 73, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 112, 73, 8)        1608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 112, 73, 8)        32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 56, 36, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_183 (Conv2D)          (None, 56, 36, 16)        3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 56, 36, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling (None, 28, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_184 (Conv2D)          (None, 28, 18, 16)        6416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 28, 18, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 14, 9, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 2016)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2016)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                32272     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 43,945\n",
      "Trainable params: 43,849\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import *\n",
    "def define_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Conv2D(8, (3, 3), padding=\"same\", activation = 'elu', input_shape=(225, 146,3)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Conv2D(8, (5, 5), padding=\"same\", activation = 'elu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Conv2D(16, (5, 5), padding=\"same\", activation = 'elu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Conv2D(16, (5, 5), padding=\"same\", activation = 'elu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Flatten(),\n",
    "            Dropout(0.5),\n",
    "            Dense(16,activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "model1 = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\", activation = 'elu', input_shape=(225, 146,3)))\n",
    "#add more layers\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2, 2))            \n",
    "model.add(Conv2D(8, (5, 5), padding=\"same\", activation = 'elu'))            \n",
    "model.add(BatchNormalization())         \n",
    "model.add(MaxPooling2D(2, 2))   \n",
    "model.add(Conv2D(16, (5, 5), padding=\"same\", activation = 'elu'))            \n",
    "model.add(BatchNormalization())           \n",
    "model.add(MaxPooling2D(2, 2))           \n",
    "model.add(Conv2D(16, (5, 5), padding=\"same\", activation = 'elu'))            \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2, 2))        \n",
    "#must flatten before the output layer\n",
    "model.add(Flatten())\n",
    "#output layer\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 180 steps, validate for 20 steps\n",
      "Epoch 1/2\n",
      "180/180 [==============================] - 153s 851ms/step - loss: 0.0725 - accuracy: 0.9243 - val_loss: 0.1504 - val_accuracy: 0.8636\n",
      "Epoch 2/2\n",
      "180/180 [==============================] - 146s 813ms/step - loss: 0.0667 - accuracy: 0.9351 - val_loss: 0.1500 - val_accuracy: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15504a5c0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x=, \n",
    "#          #params\n",
    "#          )\n",
    "# #\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "model.fit(x=train_generator, \n",
    "          validation_data=test_generator, \n",
    "          steps_per_epoch=training_len//batch_size,\n",
    "          validation_steps=testing_len//batch_size,\n",
    "          workers=8, \n",
    "          use_multiprocessing=False, \n",
    "          epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_xy_values('xpzfhhwkwb.mp4') # fake video\n",
    "res = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999993"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_xy_values('xmkwsnuzyq.mp4')\n",
    "res = model.predict(x) #real video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7104054e-07"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999\n",
      "0.9999498\n",
      "0.9999887\n",
      "0.99995327\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999964\n",
      "0.99999857\n",
      "1.0\n",
      "0.99995196\n",
      "0.9999999\n",
      "0.99999964\n",
      "0.9999999\n",
      "0.9999292\n",
      "1.0\n",
      "0.9999999\n",
      "0.9999999\n",
      "1.0\n",
      "0.9999939\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-001795bfc081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreal_videos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xy_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-184dca39c5d8>\u001b[0m in \u001b[0;36mget_xy_values\u001b[0;34m(video, video_dictionary)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get the video link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvideo_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_faces_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#check if the video is fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvideo_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-b6a104c1e5cc>\u001b[0m in \u001b[0;36mget_faces_from_video\u001b[0;34m(video_link, skipped_frames, new_max_size, face_confidence, padding, face_dim)\u001b[0m\n\u001b[1;32m     29\u001b[0m     '''\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#load the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#     frame_count = int(video.get(cv.CAP_PROP_FRAME_COUNT)) #not needed, but takes very little runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#skip appropiate number of frames based on skipped_frames input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for video in real_videos:\n",
    "    x, _ = get_xy_values(video)\n",
    "    res = model.predict(x)\n",
    "    print(res[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n",
      "[[0. 1.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-74538907e76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreal_videos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xy_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dadf2201b5d7>\u001b[0m in \u001b[0;36mget_xy_values\u001b[0;34m(video, video_dictionary)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get the video link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvideo_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_faces_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#check if the video is fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvideo_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FAKE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-4eba987c8ed6>\u001b[0m in \u001b[0;36mget_faces_from_video\u001b[0;34m(video_link, skipped_frames, new_max_size, face_confidence, padding, face_dim)\u001b[0m\n\u001b[1;32m     29\u001b[0m     '''\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#load the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#     frame_count = int(video.get(cv.CAP_PROP_FRAME_COUNT)) #not needed, but takes very little runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#skip appropiate number of frames based on skipped_frames input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for video in real_videos:\n",
    "    x, _ = get_xy_values(video)\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.engine.input_layer.Input(shape=None, batch_size=None, name=None, dtype=None, sparse=False, tensor=None, ragged=False, **kwargs)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
