{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "import tensorflow.compat.v1 as tf1\n",
    "import tensorflow_gan as tgan\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_gan as tfgan\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 as cv\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "face_detector = MTCNN()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 20,\n",
    "         'image_dims': (192, 128),\n",
    "         'noise_dims': 100,\n",
    "         'ds_size': 162770,\n",
    "         'start_epoch': 1,\n",
    "         'end_epoch': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bucket = '/Users/jeremiahherberg/Downloads/celeba-dataset/img_align_celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def input_function(params, mode=None):\n",
    "    batch_size = params['batch_size']\n",
    "    resized_height, resized_width = params['image_dims'] #s/b (192, 128)\n",
    "    noise_dims = params['noise_dims'] #this can be an arbitrary number\n",
    "    #determine if test or train split is being used\n",
    "#     shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    just_noise = (mode =='generator')\n",
    "    \n",
    "    \n",
    "\n",
    "    if just_noise:\n",
    "        #ds to generate images\n",
    "        noise_dataset = (tf.data.Dataset.from_tensors(0)\n",
    "                        .map(lambda _ : tf.random.normal([batch_size, noise_dims]))\n",
    "                        .repeat(1 if just_noise else None))\n",
    "        return noise_dataset\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    def find_face(img):\n",
    "        #add dim at the zero axis Shape will be from (x, y, z) -> (None, x, y, z)\n",
    "        image_tensor = tf.expand_dims(img, 0)\n",
    "        #undo the above line -- this is needed due to TF not allowing a filtered tensor py_function\n",
    "        image_tensor = tf.gather(image_tensor, 0)\n",
    "        #convert to array so face detector can read image\n",
    "        image_array = np.array(image_tensor)        \n",
    "        try:\n",
    "            '''If the face detector cannot detect a face, the first line in the try statement\n",
    "            will produce an IndexError. If this happens, pass the entire resized image into our model.\n",
    "            Given the small number of pictures in the DS this will apply to, the affect on the model\n",
    "            should be small, and a try/except statement should be more efficient than checking the len\n",
    "            of the number of faces detected on every photo in the DS.'''\n",
    "            x_start, y_start, x_len, y_len = face_detector.detect_faces(image_array)[0]['box']\n",
    "            #image array will only cover the detected face\n",
    "            face_array = image_array[y_start:(y_start + y_len), x_start:(x_start + x_len)]\n",
    "        except IndexError:\n",
    "            face_array = image_array\n",
    "        #resize array to match input of model\n",
    "        face_resized = tf.image.resize_with_pad(face_array,\n",
    "                                               target_height=resized_height, \n",
    "                                               target_width=resized_width)\n",
    "        #convert tensor values to between -1 and 1 (0 to 255 -> -1 to 1)\n",
    "        face_resized = (tf.cast(face_resized, tf.float32) - 127.5) / 127.5\n",
    "\n",
    "        return face_resized\n",
    "    \n",
    "\n",
    "    def find_face_dot_map_compatible(img):\n",
    "        image = img['image'] #filter\n",
    "        [image,] = tf.py_function(find_face, [image], [tf.float32])\n",
    "        return image\n",
    "    \n",
    "\n",
    "\n",
    "    image_dataset = (tfds.load('celeb_a',\n",
    "                              split='train',\n",
    "                              data_dir=ds_bucket,\n",
    "                              download=False, #tf record files should already be in the ds_bucket directory\n",
    "                              shuffle_files=True)\n",
    "                     .map(find_face_dot_map_compatible)\n",
    "                    .cache()\n",
    "                    .repeat())\n",
    "    \n",
    "    \n",
    "#     if shuffle:\n",
    "#         image_dataset = image_dataset.shuffle(buffer_size=205000, \n",
    "#                                               reshuffle_each_iteration=True)\n",
    "        \n",
    "    image_dataset = (image_dataset.batch(batch_size,\n",
    "                                        drop_remainder=True)\n",
    "                                        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return image_dataset\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_input_function(params):\n",
    "    batch_size = params['batch_size']\n",
    "    noise_dims = params['noise_dims'] #this can be an arbitrary number\n",
    "    just_noise = True\n",
    "    #ds to generate images\n",
    "\n",
    "    noise_dataset = tf.random.normal([batch_size, noise_dims])\n",
    "    return noise_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(input_shape=[192, 128, 3]):\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (5, 5), padding='same',\n",
    "                                     input_shape=input_shape)) #changed from 225*146\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.LeakyReLU()) #- examples of GANs I have found use LeakyReLU after each COnv2D layer\n",
    "#     model.add(layers.Dropout(0.0)) #look into and consider adding dropout\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    \n",
    "    model.add(layers.Conv2D(512, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "discriminator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(params=params): #178 * 218\n",
    "    input_shape = params['noise_dims']\n",
    "    input_shape = (input_shape,)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(1536, use_bias=False, input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((3, 2, 256)))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False,\n",
    "                                    activation='tanh'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #number of filters on last layer must be equal to 3 (one for each of R, G, B)\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same')) \n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "generator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model()\n",
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real_output, fake_output):\n",
    "    '''\n",
    "    takes output from a discriminator GAN model for a batch of real and fake images\n",
    "    and returns the loss for the discriminator and generator in a GAN model\n",
    "    \n",
    "    args:\n",
    "        real_output: output from a batch of real images passed through a discriminator, a tensor\n",
    "        shapped (batch_size, 1)\n",
    "        \n",
    "        fake_output: output from a batch of fake images generated by a generator GAN model, \n",
    "        passed into a discriminator GAN model, a tensor shapped (batch_size, 1)\n",
    "        \n",
    "    returns:\n",
    "        generator_loss: the generator loss for the training batch\n",
    "        \n",
    "        discriminator_loss: the discriminator loss for the training batch\n",
    "    '''\n",
    "    #I believe the entire function will need to be inside \"tpu_strategy.scope()\"\n",
    "    #review https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "    \n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    generator_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    discriminator_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    discriminator_loss_real = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "    \n",
    "    return generator_loss, discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(params, real_images):\n",
    "    generator_input = generator_input_function(params)\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        fake_images = generator(generator_input, training=True)\n",
    "\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "\n",
    "        generator_loss, discriminator_loss = loss_function(real_output, fake_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(params):\n",
    "    ds_size = params['ds_size'] #celeba DS is 162,770 images for training\n",
    "    start_epoch = params['start_epoch']\n",
    "    end_epoch = params['end_epoch']\n",
    "    batch_size = params['batch_size']\n",
    "    steps_per_epoch = int(tf.math.ceil(ds_size / batch_size))\n",
    "    \n",
    "    real_images = input_function(params)\n",
    "    start_time = time.time()\n",
    "    start_time_ = time.time()\n",
    "    \n",
    "    for epoch in np.arange(start_epoch, end_epoch + 1):\n",
    "        for step in real_images:\n",
    "            print(step.shape)\n",
    "            training_step(params, step)\n",
    "\n",
    "            \n",
    "            #display time stats every 10 epochs\n",
    "            if (epoch) % 10 == 0:\n",
    "                end_time = time.time()\n",
    "                set_time = end_time - start_time_\n",
    "                set_minutes = int(set_time / 60)\n",
    "                set_seconds = round(set_time % 60, 2)\n",
    "                total_time = end_time - start_time\n",
    "                total_hours = int(total_time / (60 * 60))\n",
    "                total_minutes = int(total_time % (60 * 60))\n",
    "                total_seconds = round(total_time % 60, 2)\n",
    "                display.clear_output()\n",
    "\n",
    "                print('Set of 10 epochs, ending in epoch {} has taken {} minutes, {} seconds'.format(epoch, \n",
    "                                                                                                     set_minutes,\n",
    "                                                                                                     set_seconds))\n",
    "                print('Time elapsed through epoch {} is {} hours, {} minutes, {} seconds').format(epoch,\n",
    "                                                                                                  total_hours,\n",
    "                                                                                                  total_minutes,\n",
    "                                                                                                  total_seconds)\n",
    "                #stop training after 4 hours\n",
    "                if total_hours >= 4:\n",
    "                    break\n",
    "            \n",
    "            #if (epoch + 1) % 50 == 0:\n",
    "                #save discriminator and generator via model.save\n",
    "                #find how how frequently want to save the model\n",
    "        \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
