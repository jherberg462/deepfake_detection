{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gan mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "import tensorflow.compat.v1 as tf1\n",
    "import tensorflow_datasets as tfds\n",
    "# import tensorflow_gan as tfgan\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    credentials=None\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file( #file location of GCS private key\n",
    "        'xx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project='deepfake-research', credentials=credentials)\n",
    "objects = client.list_blobs('celeba-ds-jh', prefix='celeba_all_preprocessed')\n",
    "tfrecords = []\n",
    "for object_ in objects:\n",
    "    path = str(object_).split(', ')[1]\n",
    "    gs_path = os.path.join('gs://celeba-ds-jh', path)\n",
    "    tfrecords.append(gs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128, #128\n",
    "         'image_dims': (192, 128),\n",
    "         'noise_dims': 100,\n",
    "         'ds_size': 202599,\n",
    "         'start_epoch': 1,\n",
    "         'end_epoch': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def input_function(params, mode=None):\n",
    "    batch_size = params['batch_size']\n",
    "    resized_height, resized_width = params['image_dims'] #s/b (192, 128)\n",
    "        \n",
    "#todo -- improve documentation \n",
    "    \n",
    "    \n",
    "    def preprocess_image(img):\n",
    "        #decode TFexample record\n",
    "        features_dictionary = {\n",
    "            'image': tf.io.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        features = tf.io.parse_single_example(img, features_dictionary)\n",
    "        decoded_image = tf.io.decode_jpeg(features['image'], 3)\n",
    "\n",
    "        #add dim at the zero axis Shape will be from (x, y, z) -> (None, x, y, z)\n",
    "        image_tensor = tf.expand_dims(decoded_image, 0)\n",
    "        #undo the above line -- this is needed due to TF not allowing a filtered tensor py_function\n",
    "        image_tensor = tf.gather(image_tensor, 0)\n",
    "\n",
    "        #convert tensor values to between -1 and 1 (0 to 255 -> -1 to 1)\n",
    "        image_tensor = (tf.cast(image_tensor, tf.float32) - 127.5) / 127.5\n",
    "\n",
    "        return image_tensor\n",
    "    \n",
    "\n",
    "    def dot_map_function(img): #no longer needed\n",
    "        [image,] = tf.py_function(preprocess_image, [img], [tf.float32])#s/b tf.float32\n",
    "        return image\n",
    "    \n",
    "\n",
    "    \n",
    "    image_dataset = (tf.data.TFRecordDataset(filenames = [tfrecords]).\n",
    "                     cache().\n",
    "                     map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).\n",
    "                     cache(). \n",
    "                     repeat())\n",
    "        \n",
    "    image_dataset = (image_dataset.batch(batch_size,\n",
    "                                        drop_remainder=True,)\n",
    "                                        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "    return image_dataset\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_input_function(params):\n",
    "    batch_size = params['batch_size']\n",
    "    noise_dims = params['noise_dims'] #this can be an arbitrary number\n",
    "#     just_noise = True\n",
    "    #ds to generate images\n",
    "\n",
    "    noise_dataset = tf.random.normal([batch_size, noise_dims])\n",
    "    return noise_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(input_shape=[192, 128, 3]):\n",
    "    \n",
    "    #consider creating a singel model that takes two inputs provides two outputs\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (5, 5), padding='same',\n",
    "                                     input_shape=input_shape)) #changed from 225*146\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.LeakyReLU()) #- examples of GANs I have found use LeakyReLU after each COnv2D layer\n",
    "#     model.add(layers.Dropout(0.0)) #look into and consider adding dropout\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(512, (5, 5), padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2, 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "# discriminator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(params=params): #178 * 218\n",
    "    input_shape = params['noise_dims']\n",
    "    input_shape = (input_shape,)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(1536, use_bias=False, input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((3, 2, 256)))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False,\n",
    "                                    activation='tanh'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #number of filters on last layer must be equal to 3 (one for each of R, G, B)\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same')) #, activation='tanh' #tanh produces poor results on this layer\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "# generator_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real_output, fake_output):\n",
    "    '''\n",
    "    takes output from a discriminator GAN model for a batch of real and fake images\n",
    "    and returns the loss for the discriminator and generator in a GAN model\n",
    "    \n",
    "    args:\n",
    "        real_output: output from a batch of real images passed through a discriminator, a tensor\n",
    "        shapped (batch_size, 1)\n",
    "        \n",
    "        fake_output: output from a batch of fake images generated by a generator GAN model, \n",
    "        passed into a discriminator GAN model, a tensor shapped (batch_size, 1)\n",
    "        \n",
    "    returns:\n",
    "        generator_loss: the generator loss for the training batch\n",
    "        \n",
    "        discriminator_loss: the discriminator loss for the training batch\n",
    "    '''\n",
    "    #I believe the entire function will need to be inside \"tpu_strategy.scope()\" (or maybe not?)\n",
    "    #review https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "    \n",
    "    # with strategy.scope():\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n",
    "    generator_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #add some noise to target values\n",
    "    extra_noise = tf.random.uniform((1,), 0, 0.1)\n",
    "    discriminator_loss_fake = cross_entropy(tf.zeros_like(fake_output) + extra_noise, fake_output)\n",
    "    discriminator_loss_real = cross_entropy(tf.ones_like(real_output) - extra_noise, real_output)\n",
    "    discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "  \n",
    "    return generator_loss, discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the models and optimizers in the strategy.scope\n",
    "with strategy.scope():\n",
    "    generator = generator_model()\n",
    "    discriminator = discriminator_model()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(params, real_images_, monitor_loss=False):\n",
    "    generator_input_ = generator_input_function(params)\n",
    "\n",
    "#make the below a function\n",
    "#inputs real_images, generator_input\n",
    "    def step(generator_input, real_images, monitor_loss_=False):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            fake_images = generator(generator_input, training=True)\n",
    "\n",
    "            real_output = discriminator(real_images, training=True)\n",
    "            fake_output = discriminator(fake_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = loss_function(real_output, fake_output)\n",
    "            # cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, \n",
    "            #                                                    reduction=tf.keras.losses.Reduction.NONE)\n",
    "            # generator_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "            # discriminator_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "            # discriminator_loss_real = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "            # discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "        \n",
    "        if monitor_loss_:\n",
    "            avg_generator_loss = tf.reduce_mean(generator_loss)\n",
    "            avg_discriminator_loss = tf.reduce_mean(discriminator_loss)\n",
    "            gen_loss.append(avg_generator_loss)\n",
    "            disc_loss.append(avg_discriminator_loss)\n",
    "\n",
    "    #run strategy.run on the above function\n",
    "    # step(generator_input_, real_images_) \n",
    "    strategy.run(step, args=(generator_input_, real_images_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_generated_pic(gen_output_tensor, return_=False):\n",
    "    '''\n",
    "    Function to convert tensor from a range of -1 to 1 -> 0 to 1 and display the resulting image\n",
    "    Will display the converted tensor as an image and can return a tensor that is converted as per above\n",
    "    '''\n",
    "    generated_image = (gen_output_tensor + 1) / 2\n",
    "    plt.imshow(generated_image)\n",
    "    plt.show()\n",
    "    if return_:\n",
    "        return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_gan(params):\n",
    "    ds_size = params['ds_size'] #celeba DS is 162,770 images for training\n",
    "    start_epoch = params['start_epoch']\n",
    "    end_epoch = params['end_epoch']\n",
    "    batch_size = params['batch_size']\n",
    "    params_ = {}\n",
    "    params_['noise_dims'] = params['noise_dims']\n",
    "\n",
    "    params_['batch_size'] = 1\n",
    "    steps_per_epoch = int(tf.math.ceil(ds_size / batch_size))\n",
    "\n",
    "\n",
    "    # train_ds = strategy.experimental_distribute_datasets_from_function(\n",
    "    #     lambda _ : input_function(params)\n",
    "    # )\n",
    "    \n",
    "    real_images = input_function(params)\n",
    "    # real_images = iter(train_ds)\n",
    "    start_time = time.time()\n",
    "    start_time_ = time.time()\n",
    "    \n",
    "    for epoch in np.arange(start_epoch, end_epoch + 1):\n",
    "        counter = 0\n",
    "        tme_start = time.time()\n",
    "        for step in real_images:\n",
    "            start = time.time()\n",
    "            training_step(params, step)\n",
    "            end = time.time()\n",
    "            tme = end - start\n",
    "            counter +=1\n",
    "            if counter % 100 == 60:\n",
    "            #remove below 3 lines once training is optimized\n",
    "            #every 100 steps, starting at step 60, display some stats and a generated image\n",
    "                display.clear_output(wait=True)\n",
    "                print('step {} took {} seconds'.format(counter, tme))\n",
    "                print('total time: {} // {} per step'. format ((end - tme_start), ((end- tme_start) / counter)))\n",
    "                gen_input = generator_input_function(params_)\n",
    "                gen_output = generator(gen_input)\n",
    "                _ = display_generated_pic(gen_output[0])\n",
    "                print(params)\n",
    "\n",
    "            \n",
    "            #end loop (finish the epoch) once the appropiate number of steps have been completed\n",
    "            if counter >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "\n",
    "            \n",
    "        #display time stats every 10 epochs\n",
    "        if (epoch) % 10 == 9:\n",
    "            end_time = time.time()\n",
    "            set_time = end_time - start_time_\n",
    "            set_minutes = int(set_time / 60)\n",
    "            set_seconds = round(set_time % 60, 2)\n",
    "            total_time = end_time - start_time\n",
    "            total_hours = int(total_time / (60 * 60))\n",
    "            total_minutes = int(total_time % (60 * 60))\n",
    "            total_seconds = round(total_time % 60, 2)\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            print('Set of 10 epochs, ending in epoch {} has taken {} minutes, {} seconds'.format(epoch, \n",
    "                                                                                                 set_minutes,\n",
    "                                                                                                 set_seconds))\n",
    "            print('Time elapsed through epoch {} is {} hours, {} minutes, {} seconds'.format(epoch,\n",
    "                                                                                              total_hours,\n",
    "                                                                                              total_minutes,\n",
    "                                                                                              total_seconds))\n",
    "            #stop training after 4 hours\n",
    "            if total_hours >= 4:\n",
    "                break\n",
    "\n",
    "        #if (epoch + 1) % 50 == 0:\n",
    "            #save discriminator and generator via model.save\n",
    "            #find how how frequently want to save the model\n",
    "        \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = params['ds_size'] #celeba DS is 162,770 images for training\n",
    "start_epoch = params['start_epoch']\n",
    "end_epoch = params['end_epoch']\n",
    "batch_size = params['batch_size']\n",
    "params_ = {}\n",
    "params_['noise_dims'] = params['noise_dims']\n",
    "\n",
    "params_['batch_size'] = 1\n",
    "steps_per_epoch = int(tf.math.ceil(ds_size / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = input_function(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training code\n",
    "step_counter = 0\n",
    "gen_loss = []\n",
    "disc_loss = []\n",
    "step = []\n",
    "# epochs = end_epoch - start_epoch \n",
    "for epoch in range(start_epoch, end_epoch + 1):\n",
    "    tme_start = time.time()\n",
    "    for image_batch in real_images:\n",
    "      # start = time.time()\n",
    "        training_step(params, image_batch)\n",
    "        end = time.time()\n",
    "        tme= end - tme_start\n",
    "        step_counter +=1\n",
    "        if step_counter %100 == 60:\n",
    "            training_step(params, image_batch, monitor_loss=True)\n",
    "            step.append(epoch * step_counter)\n",
    "            display.clear_output(wait=True)\n",
    "            print('step {}, epoch {}'.format(step_counter, epoch))\n",
    "            print('total time: {} seconds // {} per step'. format ((end - tme_start), ((end- tme_start) / step_counter)))\n",
    "            gen_input = generator_input_function(params_)\n",
    "            gen_output = generator(gen_input)\n",
    "            _ = display_generated_pic(gen_output[0])\n",
    "            print(params)\n",
    "                    #step  #loss\n",
    "            plt.plot(step, gen_loss, label='gen')\n",
    "            plt.plot(step, disc_loss, label='disc')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('step')\n",
    "            plt.legend()#('upper left')\n",
    "            plt.show()\n",
    "\n",
    "        if step_counter > steps_per_epoch:\n",
    "            step_counter = 0\n",
    "            break #end epoch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model_path = 'gs://jh-gan-testing/gan_1'\n",
    "disc_model_path = 'gs://jh-gan-testing/disc_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(gan_model_path)\n",
    "discriminator.save(disc_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
